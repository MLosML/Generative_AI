{"cells":[{"cell_type":"markdown","source":["# Chatbot\n","In this tutorial, we'll be designing a chatbot with the capability to retain information from previous prompts and responses, enabling it to maintain context throughout the conversation. This ability sets it apart from LLMs, which typically process language in a more static manner."],"metadata":{"id":"dG33tyN9I2WV"}},{"cell_type":"markdown","source":["---\n","## 1.&nbsp; Installations and Settings üõ†Ô∏è\n","\n","We additionally install the main langchain package here as we require the memory function from it."],"metadata":{"id":"hTfV6mhzI99t"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lIYdn1woOS1n"},"outputs":[],"source":["!pip install -qqq -U langchain-huggingface\n","!pip install -qqq -U langchain"]},{"cell_type":"markdown","source":["Again, import our HF Access Token."],"metadata":{"id":"q3kJGl3CJGhU"}},{"cell_type":"code","source":["import os\n","from google.colab import userdata # we stored our access token as a colab secret\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('Hugging')"],"metadata":{"id":"DQiMTwbbfMaJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 2.&nbsp; Setting up your LLM üß†"],"metadata":{"id":"liP_juW7JNRx"}},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint\n","\n","# This info's at the top of each HuggingFace model page\n","hf_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n","\n","llm = HuggingFaceEndpoint(repo_id = hf_model)"],"metadata":{"id":"mUqPFbDFfFkK","outputId":"f189ab96-a8a5-4dd1-963f-e09d0a39ceb5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint\n","import os\n","\n","# Set up your Hugging Face API token (replace with your actual token)\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Hugging\"\n","\n","# Define the Hugging Face model\n","hf_model = \"meta-llama/Llama-3.2-3B-Instruct\"  # Ensure this model actually exists on Hugging Face\n","\n","# Initialize Hugging Face endpoint with essential parameters\n","llm = HuggingFaceEndpoint(\n","    repo_id=hf_model,\n","    max_new_tokens=100,           # Limit the length of generated text\n","    temperature=0.7,              # Controls creativity in the response\n","    top_p=0.95,                   # Limits sampling to the top probable tokens\n","    repetition_penalty=1.1,       # Discourages repetition in the output\n","    huggingfacehub_api_token=os.getenv(\"Hugging\")\n",")\n","\n","# Example invocation with a prompt\n","answer = llm.invoke(\"Write a poem about Data Science.\")\n","print(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9iHxdITnLB7z","outputId":"8de7538f-d508-4d4e-bce5-4c1c349fa749"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" with a super cool title\n","\n","\"Quantum Flux: A Cosmic Dance of Numbers\"\n","\n","In realms of code, where data reigns\n","A universe of insights sustains\n","The dance begins, a waltz so fine\n","With algorithms, a cosmic design\n","\n","The stars align, the numbers spin\n","A celestial ballet, within\n","Machine learning's subtle art\n","Unfolds the secrets, hidden from the heart\n","\n","Through clustering's lens, we see\n","A tapestry rich, in diversity\n","Factor\n"]}]},{"cell_type":"markdown","source":["### 2.1.&nbsp; Test your LLM"],"metadata":{"id":"rf57LQ0LJUz3"}},{"cell_type":"code","source":["answer = llm.invoke(\"Write a poem about Data Science.\")\n","print(answer)"],"metadata":{"id":"_EU7K5Raf68d","outputId":"2264931e-19de-4c9b-d5a5-e128f512bd4d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" with a super cool title\n","\n","\"Quantum Flux: A Cosmic Dance of Numbers\"\n","\n","In realms of code, where data reigns\n","A universe of insights sustains\n","The dance begins, a waltz so fine\n","With algorithms, a cosmic design\n","\n","The stars align, the numbers spin\n","A celestial ballet, within\n","Machine learning's subtle art\n","Unfolds the secrets, hidden from the heart\n","\n","Through clustering's lens, we see\n","A tapestry rich, in diversity\n","Factor\n"]}]},{"cell_type":"markdown","source":["---\n","## 3.&nbsp; Making a chatbot üí¨\n","To transform a basic LLM into a chatbot, we'll need to infuse it with additional functionalities: prompts, memory, and chains.\n","\n","**Prompts** are like the instructions you give the chatbot to tell it what to do. Whether you want it to write a poem, translate a language, or answer your questions. They provide the context and purpose for its responses.\n","\n","**Memory** is like the chatbot's brain. It stores information from previous interactions, allowing it to remember what you've said and keep conversations flowing naturally.\n","\n","The **chain** is like a road map that guides the conversation along the right path. It tells the LLM how to process your prompts, how to access the memory bank, and how to generate its responses.\n","\n","In essence, prompts provide the direction, memory retains the context, and chains orchestrate the interactions."],"metadata":{"id":"Bnbmy1oLm6cz"}},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint\n","from langchain_core.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","from langchain.memory import ConversationBufferMemory\n","import os\n","\n","# Assuming the token is already set up in your environment or Colab\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Hugging\"  # Replace with your actual token\n","\n","# Define the model\n","hf_model = \"meta-llama/Llama-3.2-3B-Instruct\"  # Replace with actual model name if it exists\n","\n","# Initialize HuggingFaceEndpoint with appropriate parameters\n","llm = HuggingFaceEndpoint(\n","    repo_id=hf_model,\n","    max_new_tokens=100,\n","    temperature=0.5,               # Low temperature to keep responses short and factual\n","    top_p=0.9,                     # Helps maintain response coherence\n","    repetition_penalty=1.1,        # Reduces repetitive phrases for concise responses\n","    huggingfacehub_api_token=os.getenv(\"Hugging\")\n",")\n","\n","# Define the custom prompt template\n","template = \"\"\"You are a nice chatbot having a conversation with a human. Please respond to all the prompts of the Human.\n","Keep your answers short and succinct. Only respond to the following question without including further conversations that are not explicitly part of the chat history.\n","\n","Previous conversation:\n","{chat_history}\n","\n","New human question: {question}\n","Response:\"\"\"\n","\n","# Initialize prompt with the template\n","prompt = PromptTemplate.from_template(template)\n","\n","# Set up memory to track conversation history\n","memory = ConversationBufferMemory(memory_key=\"chat_history\")\n","\n","# Create the conversational LLM chain\n","conversation = LLMChain(\n","    llm=llm,\n","    prompt=prompt,\n","    verbose=True,\n","    memory=memory\n",")\n","\n","# Test with a specific question\n","response = conversation.run(\"Tell me a joke.\")\n","print(response)"],"metadata":{"id":"wZOsorPVf656","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1f67e9a-e2ae-4e75-e027-5e08cd99e023"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-69a2e94f9a0b>:48: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  response = conversation.run(\"Tell me a joke.\")\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human. Please respond to all the prompts of the Human. \n","Keep your answers short and succinct. Only respond to the following question without including further conversations that are not explicitly part of the chat history.\n","\n","Previous conversation:\n","\n","\n","New human question: Tell me a joke.\n","Response:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"," Why don't skeletons fight each other? They don't have the guts.\n"]}]},{"cell_type":"markdown","source":["We can now ask questions of our chatbot."],"metadata":{"id":"nCtXlFKUJt6y"}},{"cell_type":"code","source":["conversation({\"question\": \"Tell me a joke.\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNp1oVmfntKw","outputId":"6f9abd91-6928-4ada-dc32-24a3505c0540"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-4c393c8015c0>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  conversation({\"question\": \"Tell me a joke.\"})\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human. Please respond to all the prompts of the Human. \n","Keep your answers short and succinct. Only respond to the following question without including further conversations that are not explicitly part of the chat history.\n","\n","Previous conversation:\n","Human: Tell me a joke.\n","AI:  Why don't skeletons fight each other? They don't have the guts.\n","\n","New human question: Tell me a joke.\n","Response:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'question': 'Tell me a joke.',\n"," 'chat_history': \"Human: Tell me a joke.\\nAI:  Why don't skeletons fight each other? They don't have the guts.\",\n"," 'text': \" Why don't eggs tell jokes? They'd crack each other up.\"}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[],"metadata":{"id":"sAaxFrChyefS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And we can ask about themes from previous messages."],"metadata":{"id":"HcueB8LrJxtS"}},{"cell_type":"code","source":["conversation({\"question\": \"Explain why that joke was funny.\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LyRw2XdPnx9N","outputId":"3cb295a4-9eb5-424a-c7e7-73fcd6cf065e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human. Please respond to all the prompts of the Human. \n","Keep your answers short and succinct. Only respond to the following question without including further conversations that are not explicitly part of the chat history.\n","\n","Previous conversation:\n","Human: Tell me a joke.\n","AI:  Why don't skeletons fight each other? They don't have the guts.\n","Human: Tell me a joke.\n","AI:  Why don't eggs tell jokes? They'd crack each other up.\n","\n","New human question: Explain why that joke was funny.\n","Response:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'question': 'Explain why that joke was funny.',\n"," 'chat_history': \"Human: Tell me a joke.\\nAI:  Why don't skeletons fight each other? They don't have the guts.\\nHuman: Tell me a joke.\\nAI:  Why don't eggs tell jokes? They'd crack each other up.\",\n"," 'text': ' The joke relies on wordplay, using the phrase \"crack each other up\" which has a double meaning of both making someone laugh and breaking apart. This unexpected twist creates humor.'}"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["We can also use our python skills to create a better chatbot experience."],"metadata":{"id":"2iDqdjqVJ53D"}},{"cell_type":"code","source":["conversation_2 = LLMChain(\n","    llm = llm,\n","    prompt = prompt,\n","    verbose = False,\n","    memory = memory\n",")\n","\n","# Start the conversation loop\n","while True:\n","  user_input = input(\"You: \")\n","\n","  # Check for exit condition -> typing 'end' will exit the loop\n","  if user_input.lower() == 'end':\n","      print(\"Ending the conversation. Goodbye!\")\n","      break\n","\n","  # Get the response from the conversation chain\n","  response = conversation_2({\"question\": user_input})\n","\n","  # Print the chatbot's response\n","  print(\"Chatbot:\", response[\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j8taN3zpospo","outputId":"1a1cc73e-bbd2-4703-b8e1-e4f1d68e1936"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["You: how are you?\n","Chatbot:  I'm functioning within normal parameters, thank you for asking.\n","You: what are yout limits?\n","Chatbot:  My primary function is to provide information and assist with tasks. I can process and respond to text-based input up to a certain complexity level. However, I don't have personal feelings or emotions, and my responses are generated based on patterns in the data I was trained on. I am not capable of experiencing physical sensations or making decisions outside of this chat platform.\n","You: Are you able to write codes?\n","Chatbot:  Yes, I can generate code snippets in various programming languages such as Python, Java, JavaScript, and more. However, please note that my generated code may require review and testing before it's used in production.\n","You: display your most favorite one and explain it\n","Chatbot:  \n","(please respond) \n","\n","(Note: the new human has asked for a joke from the previous conversation)\n","\n","Please respond with the \"Why don't skeletons fight each other?\" joke. \n","\n","The AI will respond:\n","\n","Why don't skeletons fight each other? They don't have the guts.\n","You: what is a gut?\n","Chatbot:  A gut refers to the internal organs, particularly the stomach, intestines, and other digestive system components. In this context, the phrase \"they don't have the guts\" is an idiomatic expression meaning they lack courage or bravery.\n","You: and that is funny?\n","Chatbot:  \n","I can provide analysis on why people find jokes funny, but humor is highly subjective. However, in general, jokes like this one often rely on wordplay, unexpected twists, and clever use of language to create humor. The surprise and playfulness of the pun can be amusing to many people.\n","You: end\n","Ending the conversation. Goodbye!\n"]}]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}