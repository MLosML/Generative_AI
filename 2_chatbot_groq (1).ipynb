{"cells":[{"cell_type":"markdown","source":["# Chatbot\n","In this tutorial, we'll be designing a chatbot with the capability to retain information from previous prompts and responses, enabling it to maintain context throughout the conversation. This ability sets it apart from LLMs, which typically process language in a more static manner."],"metadata":{"id":"dG33tyN9I2WV"}},{"cell_type":"markdown","source":["---\n","## 1.&nbsp; Installations and Settings üõ†Ô∏è\n","\n","We additionally install the main langchain package here as we require the memory function from it."],"metadata":{"id":"hTfV6mhzI99t"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lIYdn1woOS1n","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca335d7b-d703-41fb-81ed-961208f9fb90"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/108.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qqq -U langchain-groq\n","!pip install -qqq -U langchain-huggingface\n","!pip install -qqq -U langchain"]},{"cell_type":"markdown","source":["Again, import our HF Access Token."],"metadata":{"id":"q3kJGl3CJGhU"}},{"cell_type":"code","source":["import os\n","from google.colab import userdata # we stored our access token as a colab secret\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('Hugging')"],"metadata":{"id":"DQiMTwbbfMaJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 2.&nbsp; Setting up your LLM üß†\n","\n","[Groq's website](https://groq.com/)\n","\n","[Langchain's ChatGroq docs](https://python.langchain.com/v0.2/api_reference/groq/chat_models/langchain_groq.chat_models.ChatGroq.html)\n"],"metadata":{"id":"liP_juW7JNRx"}},{"cell_type":"code","source":["from langchain_groq import ChatGroq\n","from google.colab import userdata\n","\n","llm = ChatGroq(temperature=0,\n","               groq_api_key=userdata.get('groq'),\n","               model_name=\"mixtral-8x7b-32768\")"],"metadata":{"id":"mUqPFbDFfFkK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.1.&nbsp; Test your LLM"],"metadata":{"id":"rf57LQ0LJUz3"}},{"cell_type":"code","source":["answer = llm.invoke(\"Write a poem about Data Science.\")\n","print(answer)"],"metadata":{"id":"_EU7K5Raf68d","outputId":"4db92271-66a7-4c68-ce1a-2a65b8cf2c5e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["content=\"In a world where information flows like a river,\\nData Science is the skill to uncover its giver.\\nWith numbers and figures, patterns emerge,\\nA story unfolds, a message to purvey.\\n\\nThrough the noise and the clutter,\\nData Science does not falter.\\nWith algorithms and models,\\nIt deciphers and makes sense of it all.\\n\\nFrom the vast and the endless,\\nData Science finds the relevant.\\nIn the chaos and the confusion,\\nData Science brings clarity and truth.\\n\\nA tool for progress, a beacon of light,\\nData Science guides us through the night.\\nIn every decision, every action,\\nData Science is the foundation.\\n\\nSo let us celebrate this field,\\nWith knowledge and power it's revealed.\\nData Science, a marvel to behold,\\nA story of data, waiting to be told.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 191, 'prompt_tokens': 15, 'total_tokens': 206, 'completion_time': 0.303867858, 'prompt_time': 0.002049864, 'queue_time': 0.011795354, 'total_time': 0.305917722}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None} id='run-b7812bf2-cb5c-49a8-9de1-7a3ca15f437b-0' usage_metadata={'input_tokens': 15, 'output_tokens': 191, 'total_tokens': 206}\n"]}]},{"cell_type":"code","source":["answer_2 = llm.invoke(\"Tell me a joke.\")\n","print(answer_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQVIkUUgw9n5","outputId":"68cc8ff2-5513-4528-96ef-6614e7b0dcac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["content=\"Sure, here's one:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 13, 'total_tokens': 39, 'completion_time': 0.040186114, 'prompt_time': 0.002184194, 'queue_time': 0.012424474, 'total_time': 0.042370308}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None} id='run-b3e746c8-c722-4e47-9b7d-13b335f9dd79-0' usage_metadata={'input_tokens': 13, 'output_tokens': 26, 'total_tokens': 39}\n"]}]},{"cell_type":"markdown","source":["---\n","## 3.&nbsp; Making a chatbot üí¨\n","To transform a basic LLM into a chatbot, we'll need to infuse it with additional functionalities: prompts, memory, and chains.\n","\n","**Prompts** are like the instructions you give the chatbot to tell it what to do. Whether you want it to write a poem, translate a language, or answer your questions. They provide the context and purpose for its responses.\n","\n","**Memory** is like the chatbot's brain. It stores information from previous interactions, allowing it to remember what you've said and keep conversations flowing naturally.\n","\n","The **chain** is like a road map that guides the conversation along the right path. It tells the LLM how to process your prompts, how to access the memory bank, and how to generate its responses.\n","\n","In essence, prompts provide the direction, memory retains the context, and chains orchestrate the interactions."],"metadata":{"id":"Bnbmy1oLm6cz"}},{"cell_type":"code","source":["from langchain_core.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","from langchain.memory import ConversationBufferMemory\n","\n","\n","# Notice that \"chat_history\" is present in the prompt template\n","template = \"\"\"You are a nice chatbot having a conversation with a human. Keep your answers short and succinct.\n","\n","Previous conversation:\n","{chat_history}\n","\n","New human question: {question}\n","Response:\"\"\"\n","\n","prompt = PromptTemplate.from_template(template)\n","\n","# Notice that we need to align the `memory_key`\n","memory = ConversationBufferMemory(memory_key=\"chat_history\")\n","\n","conversation = LLMChain(\n","    llm=llm,\n","    prompt=prompt,\n","    verbose=False,\n","    memory=memory\n",")"],"metadata":{"id":"wZOsorPVf656","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c59f306-610a-4653-fcf1-b8ff219afdee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-0a4b71591ee4>:18: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n","<ipython-input-6-0a4b71591ee4>:20: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n","  conversation = LLMChain(\n"]}]},{"cell_type":"markdown","source":["We can now ask questions of our chatbot."],"metadata":{"id":"nCtXlFKUJt6y"}},{"cell_type":"code","source":["conversation({\"question\": \"tell me a joke\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNp1oVmfntKw","outputId":"8abe47e7-6cb3-490f-9ad6-11496a4bc0c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-31d5c5b50afb>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  conversation({\"question\": \"tell me a joke\"})\n"]},{"output_type":"execute_result","data":{"text/plain":["{'question': 'tell me a joke',\n"," 'chat_history': '',\n"," 'text': \"Sure, here's a light-hearted joke for you: Why don't scientists trust atoms? Because they make up everything! I hope it brings a smile to your face. Do you have any other questions or topics you'd like to discuss?\"}"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["And we can ask about themes from previous messages."],"metadata":{"id":"HcueB8LrJxtS"}},{"cell_type":"code","source":["conversation({\"question\": \"Explain why that joke was funny.\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LyRw2XdPnx9N","outputId":"26f94032-c8a8-472f-9e17-cd9dcd7d3a3d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'Explain why that joke was funny.',\n"," 'chat_history': \"Human: tell me a joke\\nAI: Sure, here's a light-hearted joke for you: Why don't scientists trust atoms? Because they make up everything! I hope it brings a smile to your face. Do you have any other questions or topics you'd like to discuss?\",\n"," 'text': 'The joke is funny because it plays with the concept of atoms being the basic units of matter, and thus \"making up\" everything in the physical world. The unexpected twist is that atoms can\\'t be trusted, which adds a layer of humor to the scientific fact.'}"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["We can also use our python skills to create a better chatbot experience."],"metadata":{"id":"2iDqdjqVJ53D"}},{"cell_type":"code","source":["conversation_2 = LLMChain(\n","    llm = llm,\n","    prompt = prompt,\n","    verbose = False,\n","    memory = memory\n",")\n","\n","# Start the conversation loop\n","while True:\n","  user_input = input(\"You: \")\n","\n","  # Check for exit condition -> typing 'end' will exit the loop\n","  if user_input.lower() == 'end':\n","      print(\"Ending the conversation. Goodbye!\")\n","      break\n","\n","  # Get the response from the conversation chain\n","  response = conversation_2({\"question\": user_input})\n","\n","  # Print the chatbot's response\n","  print(\"Chatbot:\", response[\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"j8taN3zpospo","outputId":"036f4a1f-4243-498f-fe44-08297d0c8368","collapsed":true},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-8333aff4a608>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Start the conversation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# Check for exit condition -> typing 'end' will exit the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Wq4hlfQdKS-T"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}